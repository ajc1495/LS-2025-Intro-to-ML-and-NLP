{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e12b9b9-b62e-4f4e-9ef3-fe3da109c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed safetensors-0.5.3 tokenizers-0.21.2 transformers-4.53.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8abe9a80-aeb7-486e-8393-5cc7c8f8445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: dill in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mahathinakka/virtual_env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a675b76a-faed-4cad-83dd-5a7ab57e4bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Next Word Predictor using Transformers\n",
      "Initializing predictor...\n",
      "Loading wikitext dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7aa8ee6c6a42e58a8279ec109d9749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200c460f547d42029e0b6bd8fed89a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667eac70ac8247aa897d89a17e9cd6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bac9e1903004237b07fa80697a7218c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788dc54b0d6f4f7592fa183d17b7b2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d4a8858be34469a4b74eb18d727674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Train samples: 36718\n",
      "Validation samples: 3760\n",
      "Test samples: 4358\n",
      "\n",
      " Next Word Prediction Demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: 'The weather today is'\n",
      "Top 5 predictions:\n",
      "  1. ' very' (probability: 0.0982)\n",
      "  2. ' good' (probability: 0.0655)\n",
      "  3. ' pretty' (probability: 0.0608)\n",
      "  4. ' a' (probability: 0.0592)\n",
      "  5. ' not' (probability: 0.0463)\n",
      "\n",
      "Input: 'Machine learning is a'\n",
      "Top 5 predictions:\n",
      "  1. ' very' (probability: 0.1373)\n",
      "  2. ' great' (probability: 0.0949)\n",
      "  3. ' big' (probability: 0.0461)\n",
      "  4. ' new' (probability: 0.0424)\n",
      "  5. ' powerful' (probability: 0.0403)\n",
      "\n",
      "Input: 'Python programming language'\n",
      "Top 5 predictions:\n",
      "  1. '.' (probability: 0.5082)\n",
      "  2. ',' (probability: 0.2904)\n",
      "  3. ' is' (probability: 0.0422)\n",
      "  4. ' and' (probability: 0.0288)\n",
      "  5. ' that' (probability: 0.0171)\n",
      "\n",
      "Input: 'The quick brown fox'\n",
      "Top 5 predictions:\n",
      "  1. 'es' (probability: 0.3929)\n",
      "  2. ' was' (probability: 0.0885)\n",
      "  3. ' is' (probability: 0.0590)\n",
      "  4. ''s' (probability: 0.0560)\n",
      "  5. ',' (probability: 0.0516)\n",
      "\n",
      " Text Generation Demo \n",
      "\n",
      "Prompt: 'The future of artificial intelligence'\n",
      "Generated: 'The future of artificial intelligence, the future of the whole society, is in the hands of the people. The future of the whole society is in the hands of the people.\n",
      "\n",
      "The future of reality, the future of the whole society, is'\n",
      "\n",
      "Prompt: 'In the world of technology'\n",
      "Generated: 'In the world of technology, there are many different things to do, but I believe that it is important to remember that this is not a new concept, but rather a unique phenomenon that has existed for some time. The purpose of this article is to'\n",
      "\n",
      "Prompt: 'Climate change is'\n",
      "Generated: 'Climate change is occurring with the planet's temperature, and the rapid rise in sea levels has prompted scientists to draw conclusions about climate change and its impact on human health.\n",
      "\n",
      "But Dr. Pfeiffer says scientists are still not sure that humans'\n",
      "\n",
      " Model Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Calculating perplexity: 100%|███████████████████| 25/25 [00:09<00:00,  2.74it/s]\n",
      "Calculating top-1 accuracy: 100%|███████████████| 25/25 [10:50<00:00, 26.02s/it]\n",
      "Calculating top-5 accuracy: 100%|███████████████| 25/25 [11:00<00:00, 26.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 79.5863\n",
      "Test Top-1 Accuracy: 0.0928\n",
      "Test Top-5 Accuracy: 0.1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    AutoTokenizer,\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NextWordPredictor:\n",
    "    def __init__(self, model_name=\"gpt2\", max_length=128):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def load_and_preprocess_data(self, dataset_name=\"wikitext\", dataset_config=\"wikitext-2-raw-v1\"):\n",
    "        print(f\"Loading {dataset_name} dataset...\")\n",
    "        \n",
    "        \n",
    "        if dataset_name == \"wikitext\":\n",
    "            dataset = load_dataset(\"wikitext\", dataset_config)\n",
    "        else: \n",
    "            dataset = load_dataset(dataset_name, dataset_config)\n",
    "        \n",
    "      \n",
    "        def tokenize_function(examples):\n",
    "            tokenized = self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "            \n",
    "            return tokenized\n",
    "        \n",
    "     \n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset[\"train\"].column_names\n",
    "        )\n",
    "        \n",
    "       \n",
    "        tokenized_dataset = tokenized_dataset.filter(\n",
    "            lambda x: len(x[\"input_ids\"]) > 1\n",
    "        )\n",
    "        \n",
    "        self.train_dataset = tokenized_dataset[\"train\"]\n",
    "        self.val_dataset = tokenized_dataset[\"validation\"]\n",
    "        self.test_dataset = tokenized_dataset[\"test\"]\n",
    "        \n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        print(f\"Train samples: {len(self.train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(self.val_dataset)}\")\n",
    "        print(f\"Test samples: {len(self.test_dataset)}\")\n",
    "        \n",
    "    def fine_tune_model(self, output_dir=\"./fine_tuned_gpt2\", epochs=3, batch_size=8):\n",
    "        \n",
    "        print(\"Starting fine-tuning...\")\n",
    "        \n",
    "      \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=500,\n",
    "            save_steps=1000,\n",
    "            warmup_steps=100,\n",
    "            logging_steps=100,\n",
    "            prediction_loss_only=True,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            report_to=None,  #\n",
    "        )\n",
    "        \n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,  \n",
    "        )\n",
    "        \n",
    "     \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "       \n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"Fine-tuning completed! Model saved to {output_dir}\")\n",
    "        \n",
    "    def calculate_perplexity(self, dataset, batch_size=8):\n",
    "        \n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "      \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item() * input_ids.size(0)\n",
    "                total_tokens += input_ids.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "        \n",
    "        return perplexity.item()\n",
    "    \n",
    "    def calculate_top_k_accuracy(self, dataset, k=5, batch_size=8):\n",
    "        self.model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=f\"Calculating top-{k} accuracy\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "               \n",
    "                for i in range(input_ids.size(1) - 1):\n",
    "                    input_seq = input_ids[:, :i+1]\n",
    "                    target_token = input_ids[:, i+1]\n",
    "                    \n",
    "                    outputs = self.model(input_seq)\n",
    "                    logits = outputs.logits[:, -1, :]  # Get last token logits\n",
    "                    \n",
    "                    \n",
    "                    top_k_tokens = torch.topk(logits, k, dim=-1).indices\n",
    "                    \n",
    "                    correct_predictions += (target_token.unsqueeze(1) == top_k_tokens).any(dim=1).sum().item()\n",
    "                    total_predictions += input_ids.size(0)\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        return accuracy\n",
    "    \n",
    "    def predict_next_word(self, text, num_predictions=5, temperature=0.7):\n",
    "        self.model.eval()\n",
    "        \n",
    "       \n",
    "        inputs = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            predictions = outputs.logits[0, -1, :]\n",
    "            \n",
    "            \n",
    "            predictions = predictions / temperature\n",
    "            \n",
    "            \n",
    "            probabilities = torch.softmax(predictions, dim=-1)\n",
    "            \n",
    "           \n",
    "            top_predictions = torch.topk(probabilities, num_predictions)\n",
    "            \n",
    "            results = []\n",
    "            for i in range(num_predictions):\n",
    "                token_id = top_predictions.indices[i].item()\n",
    "                probability = top_predictions.values[i].item()\n",
    "                word = self.tokenizer.decode([token_id])\n",
    "                results.append((word, probability))\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    def generate_text(self, prompt, max_length=50, temperature=0.7, do_sample=True):\n",
    "        self.model.eval()\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \n",
    "        print(\"Evaluating model...\")\n",
    "        \n",
    "     \n",
    "        train_perplexity = self.calculate_perplexity(self.train_dataset)\n",
    "        val_perplexity = self.calculate_perplexity(self.val_dataset)\n",
    "        test_perplexity = self.calculate_perplexity(self.test_dataset)\n",
    "        \n",
    "     \n",
    "        train_top1_acc = self.calculate_top_k_accuracy(self.train_dataset, k=1)\n",
    "        train_top5_acc = self.calculate_top_k_accuracy(self.train_dataset, k=5)\n",
    "        \n",
    "        val_top1_acc = self.calculate_top_k_accuracy(self.val_dataset, k=1)\n",
    "        val_top5_acc = self.calculate_top_k_accuracy(self.val_dataset, k=5)\n",
    "        \n",
    "        test_top1_acc = self.calculate_top_k_accuracy(self.test_dataset, k=1)\n",
    "        test_top5_acc = self.calculate_top_k_accuracy(self.test_dataset, k=5)\n",
    "        \n",
    "        results = {\n",
    "            'perplexity': {\n",
    "                'train': train_perplexity,\n",
    "                'validation': val_perplexity,\n",
    "                'test': test_perplexity\n",
    "            },\n",
    "            'top1_accuracy': {\n",
    "                'train': train_top1_acc,\n",
    "                'validation': val_top1_acc,\n",
    "                'test': test_top1_acc\n",
    "            },\n",
    "            'top5_accuracy': {\n",
    "                'train': train_top5_acc,\n",
    "                'validation': val_top5_acc,\n",
    "                'test': test_top5_acc\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, results):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "   \n",
    "        datasets = ['train', 'validation', 'test']\n",
    "        perplexities = [results['perplexity'][d] for d in datasets]\n",
    "        \n",
    "        axes[0].bar(datasets, perplexities, color=['blue', 'orange', 'green'])\n",
    "        axes[0].set_title('Perplexity by Dataset')\n",
    "        axes[0].set_ylabel('Perplexity')\n",
    "        \n",
    "  \n",
    "        top1_accs = [results['top1_accuracy'][d] for d in datasets]\n",
    "        axes[1].bar(datasets, top1_accs, color=['blue', 'orange', 'green'])\n",
    "        axes[1].set_title('Top-1 Accuracy by Dataset')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        \n",
    "  \n",
    "        top5_accs = [results['top5_accuracy'][d] for d in datasets]\n",
    "        axes[2].bar(datasets, top5_accs, color=['blue', 'orange', 'green'])\n",
    "        axes[2].set_title('Top-5 Accuracy by Dataset')\n",
    "        axes[2].set_ylabel('Accuracy')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "   \n",
    "    print(\" Next Word Predictor using Transformers\")\n",
    "    print(\"Initializing predictor...\")\n",
    "    \n",
    "    \n",
    "    predictor = NextWordPredictor(model_name=\"gpt2\", max_length=128)\n",
    "    \n",
    "  \n",
    "    predictor.load_and_preprocess_data()\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    # uncomment to fine tune - it takes a lot of time\n",
    "    # predictor.fine_tune_model(epochs=1, batch_size=4) \n",
    "    \n",
    "  \n",
    "    print(\"\\n Next Word Prediction Demo\")\n",
    "    test_sentences = [\n",
    "        \"The weather today is\",\n",
    "        \"Machine learning is a\",\n",
    "        \"Python programming language\",\n",
    "        \"The quick brown fox\"\n",
    "    ]\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        predictions = predictor.predict_next_word(sentence, num_predictions=5)\n",
    "        print(f\"\\nInput: '{sentence}'\")\n",
    "        print(\"Top 5 predictions:\")\n",
    "        for i, (word, prob) in enumerate(predictions, 1):\n",
    "            print(f\"  {i}. '{word}' (probability: {prob:.4f})\")\n",
    "    \n",
    "   \n",
    "    print(\"\\n Text Generation Demo \")\n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence\",\n",
    "        \"In the world of technology\",\n",
    "        \"Climate change is\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generated = predictor.generate_text(prompt, max_length=50)\n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        print(f\"Generated: '{generated}'\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n Model Evaluation\")\n",
    "    \n",
    "    small_test = predictor.test_dataset.select(range(min(100, len(predictor.test_dataset))))\n",
    "    \n",
    "    test_perplexity = predictor.calculate_perplexity(small_test, batch_size=4)\n",
    "    test_top1_acc = predictor.calculate_top_k_accuracy(small_test, k=1, batch_size=4)\n",
    "    test_top5_acc = predictor.calculate_top_k_accuracy(small_test, k=5, batch_size=4)\n",
    "    \n",
    "    print(f\"Test Perplexity: {test_perplexity:.4f}\")\n",
    "    print(f\"Test Top-1 Accuracy: {test_top1_acc:.4f}\")\n",
    "    print(f\"Test Top-5 Accuracy: {test_top5_acc:.4f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae54ec7-b009-404c-85bd-044a2988ceeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
